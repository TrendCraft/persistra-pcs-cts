/**
 * Memory Graph Initializer
 * 
 * Initializes the memory graph with core knowledge about semantic search
 * capabilities, memory graph concepts, and related components.
 */

const fs = require('fs');
const path = require('path');
const crypto = require('crypto');
const { createComponentLogger } = require('./logger');
const eventBus = require('./event-bus');

const logger = createComponentLogger('memory-graph-initializer');

/**
 * Initializes the memory graph with core knowledge about semantic search
 */
async function initializeMemoryGraph() {
  const chunksPath = path.join(process.cwd(), 'data', 'chunks.jsonl');
  const embeddingsPath = path.join(process.cwd(), 'data', 'embeddings.jsonl');
  
  // Create directories if they don't exist
  const chunksDir = path.dirname(chunksPath);
  const embeddingsDir = path.dirname(embeddingsPath);
  
  if (!fs.existsSync(chunksDir)) fs.mkdirSync(chunksDir, { recursive: true });
  if (!fs.existsSync(embeddingsDir)) fs.mkdirSync(embeddingsDir, { recursive: true });
  
  // Core knowledge chunks about semantic search
  const knowledgeChunks = [
    {
      id: crypto.randomUUID(),
      text: `Local Semantic Search Implementation: Leo uses a vector-based semantic search system that operates entirely locally. The system loads chunks of text and their corresponding embeddings from files at ${chunksPath} and ${embeddingsPath}. When a query is received, it generates an embedding for the query and calculates cosine similarity with stored embeddings to find the most relevant chunks.`
    },
    {
      id: crypto.randomUUID(),
      text: `Semantic Search Algorithm: The search process involves: 1) Loading text chunks and embeddings from disk, 2) Generating an embedding for the query text, 3) Computing cosine similarity between the query embedding and all stored embeddings, 4) Ranking results by similarity score, 5) Returning chunks above the similarity threshold (default: 0.65).`
    },
    {
      id: crypto.randomUUID(),
      text: `Memory Graph Structure: Leo's memory graph consists of text chunks and their vector embeddings. Each chunk has a unique ID that links it to its embedding. The system supports both JSONL storage and binary storage formats for embeddings, with configuration options to control batch sizes, buffer intervals, and maximum results.`
    },
    {
      id: crypto.randomUUID(),
      text: `Search Commands: To perform a semantic search, use memory.search("your query here"). To retrieve specific chunks by ID, use memory.getChunkById("chunk-id"). To get recent conversations, use memory.getRecentConversations(5).`
    },
    {
      id: crypto.randomUUID(),
      text: `Error Handling: If embeddings are missing, the system will fall back to keyword search. If no results are found, acknowledge this and use general knowledge. The system logs all search operations for debugging purposes.`
    },
    {
      id: crypto.randomUUID(),
      text: `File Structure: Chunks are stored at data/chunks.jsonl in format: {"id": "uuid", "text": "content"}. Embeddings are stored at data/embeddings.jsonl in format: {"id": "uuid", "embedding": [float array]}.`
    },
    {
      id: crypto.randomUUID(),
      text: `Vector Similarity: Leo uses cosine similarity to compare embeddings. Cosine similarity measures the cosine of the angle between two vectors, with values ranging from -1 (opposite) to 1 (identical). The system considers chunks with similarity above 0.65 as relevant.`
    }
  ];
  
  // Generate mock embeddings (in real system, these would be generated by an embedding model)
  const embeddings = knowledgeChunks.map(chunk => ({
    id: chunk.id,
    embedding: Array(384).fill(0).map(() => (Math.random() * 2) - 1) // 384-dimensional random unit vectors
  }));
  
  // Check if files already exist and have content
  const chunksExist = fs.existsSync(chunksPath) && fs.statSync(chunksPath).size > 0;
  const embeddingsExist = fs.existsSync(embeddingsPath) && fs.statSync(embeddingsPath).size > 0;

  // If both files exist and have content, don't overwrite them
  if (chunksExist && embeddingsExist) {
    logger.info('Memory graph files already exist and have content, skipping initialization');
    return true;
  }

  // Create backup of existing files if they exist
  if (chunksExist) {
    const backupPath = `${chunksPath}.backup.${Date.now()}`;
    fs.copyFileSync(chunksPath, backupPath);
    logger.info(`Created backup of existing chunks file at ${backupPath}`);
  }

  if (embeddingsExist) {
    const backupPath = `${embeddingsPath}.backup.${Date.now()}`;
    fs.copyFileSync(embeddingsPath, backupPath);
    logger.info(`Created backup of existing embeddings file at ${backupPath}`);
  }

  // Write chunks and embeddings to files
  try {
    // Write to temporary files first
    const tempChunksPath = `${chunksPath}.temp`;
    const tempEmbeddingsPath = `${embeddingsPath}.temp`;
    
    fs.writeFileSync(tempChunksPath, knowledgeChunks.map(chunk => JSON.stringify(chunk)).join('\n'));
    fs.writeFileSync(tempEmbeddingsPath, embeddings.map(emb => JSON.stringify(emb)).join('\n'));
    
    // Atomic rename to avoid partial writes
    fs.renameSync(tempChunksPath, chunksPath);
    fs.renameSync(tempEmbeddingsPath, embeddingsPath);
    
    logger.info(`Successfully initialized memory graph with ${knowledgeChunks.length} core knowledge chunks`);
    eventBus.emit('memory-graph:initialized', { count: knowledgeChunks.length });
    return true;
  } catch (error) {
    logger.error(`Failed to initialize memory graph: ${error.message}`);
    return false;
  }
}

module.exports = { initializeMemoryGraph };
